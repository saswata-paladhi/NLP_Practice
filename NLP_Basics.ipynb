{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp= spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"We'r are moving to L.A.!\"\n"
     ]
    }
   ],
   "source": [
    "myString= '\"We\\'r are moving to L.A.!\"'\n",
    "print(myString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc= nlp(myString)          #Tokenization\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "We'r\n",
      "are\n",
      "moving\n",
      "to\n",
      "L.A.\n",
      "!\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2= nlp(u\"We'r here to help. Reach us at saswata@onlyfans.com.\")                  #Here u stands for unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We'r\n",
      "here\n",
      "to\n",
      "help\n",
      ".\n",
      "Reach\n",
      "us\n",
      "at\n",
      "saswata@onlyfans.com\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for i in doc2:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "doc3= nlp(u\"A 5km ride costs me $11.34.\")\n",
    "for i in doc3:\n",
    "    print(type(i.text))\n",
    "print(doc3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123,223\n"
     ]
    }
   ],
   "source": [
    "n= nlp(u'123,223')\n",
    "for i in n:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "len(nlp.vocab)\n",
    "print(type(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are\n"
     ]
    }
   ],
   "source": [
    "print(doc[2])                   #We cant perform reassigment in tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5km\n",
      "QUANTITY\n",
      "Measurements, as of weight or distance\n",
      "11.34\n",
      "MONEY\n",
      "Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "for e in doc3.ents:                     #This is for recog entities. Entities in spacy are real world objects.\n",
    "    print(e)\n",
    "    print(e.label_)\n",
    "    print(str(spacy.explain(e.label_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We'r\n",
      "L.A.\n"
     ]
    }
   ],
   "source": [
    "for noun in doc.noun_chunks:\n",
    "    print(noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "driving------->drive \n",
      "taken------->taken \n",
      "reincarnation------->reincarn \n",
      "fairly------->fairli \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "p_stemmer= PorterStemmer()\n",
    "words= ['driving', 'taken', 'reincarnation', 'fairly']\n",
    "for word in words:\n",
    "    print(f'{word}------->{p_stemmer.stem(word)} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "s_stemmer= SnowballStemmer(language= 'english')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "driving------->drive \n",
      "taken------->taken \n",
      "reincarnation------->reincarn \n",
      "fairly------->fair \n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(f'{word}------->{s_stemmer.stem(word)} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"         \t15884554869126768810 \n",
      "we'r      \t14688043157201927837 \n",
      "be        \t10382539506755952630 \n",
      "move      \t13534686644065735227 \n",
      "to        \t3791531372978436496 \n",
      "L.A.      \t11715335255722627455 \n",
      "!         \t17494803046312582752 \n",
      "\"         \t15884554869126768810 \n"
     ]
    }
   ],
   "source": [
    "for tok in doc:\n",
    "    print(f'{tok.lemma_:10}\\t{tok.lemma} ')            #The numbers in tok.lemma is the hjash reference of the lemma in eng library of spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.vocab.Vocab"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['as'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words.add('btw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['btw'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "st= nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print('btw' in nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['not'].is_stop= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher= Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SolarPower, Solar-Power, Solar Power. Three different interations of same semantic meaning string.\n",
    "pattern_1= [{'LOWER':'solarpower'}]\n",
    "pattern_2= [{'LOWER':'solar'}, {'IS_PUNCT':True}, {'LOWER':'power'}]\n",
    "pattern_3= [{'LOWER':'solar'},{'LOWER':'power'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add('SolarPower', [pattern_1, pattern_2, pattern_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "Solar\n",
      "Power\n",
      "industry\n",
      "be\n",
      "grow\n",
      ".\n",
      "SolarPower\n",
      "be\n",
      "the\n",
      "upcoming\n",
      "company\n",
      "gain\n",
      "sale\n",
      ".\n",
      "solar\n",
      "--\n",
      "power\n",
      "will\n",
      "be\n",
      "the\n",
      "future\n",
      "energy\n",
      "source\n",
      ".\n",
      "solar\n",
      "...\n",
      "power\n"
     ]
    }
   ],
   "source": [
    "doc_solar_power= nlp(u\"The Solar Power industry is growing. SolarPower is the upcoming company gaining sales. Solar--Power will be the future energy source. Solar...Power\")\n",
    "for token in doc_solar_power:\n",
    "    token= token.lemma_\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SolarPower---8656102463236116519---Solar Power \n",
      "SolarPower---8656102463236116519---SolarPower \n",
      "SolarPower---8656102463236116519---Solar--Power \n",
      "SolarPower---8656102463236116519---Solar...Power \n"
     ]
    }
   ],
   "source": [
    "matches= matcher(doc_solar_power)\n",
    "def match_print(matches, doc):\n",
    "    for match_id, start, end in matches:\n",
    "        string_id= nlp.vocab.strings[match_id]\n",
    "        og_text= doc[start:end]\n",
    "        print(f'{string_id}---{match_id}---{og_text} ')\n",
    "match_print(matches, doc_solar_power)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.remove('SolarPower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_1= [{'LOWER':'solarpower'}]\n",
    "pattern_2= [{'LOWER':'solar'}, {'IS_PUNCT':True, 'OP':'*'}, {'LOWER':'power'}]\n",
    "pattern_3= [{'LOWER':'solar'},{'LOWER':'power'}]\n",
    "matcher.add('SolarPower', [pattern_1, pattern_2, pattern_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SolarPower---8656102463236116519---Solar Power \n",
      "SolarPower---8656102463236116519---SolarPower \n",
      "SolarPower---8656102463236116519---Solar--Power \n",
      "SolarPower---8656102463236116519---Solar...Power \n"
     ]
    }
   ],
   "source": [
    "match_print(matcher(doc_solar_power), doc_solar_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "matcher= PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\saswa\\saswata\\INTERMEDIATE_NLP_COURSE\\TextFiles\\reaganomics.txt') as f:\n",
    "    doc= nlp(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_list= ['voodoo economics', 'supply economics', 'trickle-down economics', 'free-market economics', 'supply-side economics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_pattern= [nlp(text) for text in phrase_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add('EcoMatcher',  None, *phrase_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2351661100535932681, 41, 45), (2351661100535932681, 49, 53), (2351661100535932681, 54, 56), (2351661100535932681, 61, 65), (2351661100535932681, 673, 677), (2351661100535932681, 2987, 2991)]\n"
     ]
    }
   ],
   "source": [
    "matches= matcher(doc)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b83752dd2351e549d4486d00579c2754387b6552781a235cfc636d34e5e38ea9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
